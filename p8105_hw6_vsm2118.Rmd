---
title: "Homework 6"
author: "Vicky Mello"
date: "November 30, 2023" 
output: github_document
---


```{r load_libraries}
library(tidyverse)
library(modelr)
```


### Problem 1

In the data cleaning code below we create a `city_state` variable, change `victim_age` to numeric, modifiy victim_race to have categories white and non-white, with white as the reference category, and create a `resolution` variable indicating whether the homicide is solved. Lastly, we filtered out the following cities: Tulsa, AL; Dallas, TX; Phoenix, AZ; and Kansas City, MO; and we retained only the variables `city_state`, `resolution`, `victim_age`, `victim_sex`, and `victim_race`.

```{r q1_data_cleaning}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) |> 
  filter(victim_race %in% c("White", "Black")) |> 
  filter(!(city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO"))) |> 
  select(city_state, resolution, victim_age, victim_sex, victim_race)
```

Next we fit a logistic regression model using only data from Baltimore, MD. We model `resolved` as the outcome and `victim_age`, `victim_sex`, and `victim_race` as predictors. We save the output as `baltimore_glm` so that we can apply `broom::tidy` to this object and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing non-white victims to white victims.

```{r q1_glm_baltimore}
baltimore_glm = 
  filter(homicide_df, city_state == "Baltimore, MD") |> 
  glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = _)

baltimore_glm |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(OR, OR_CI_lower, OR_CI_upper) |>
  knitr::kable(digits = 3)
```

Below, by incorporating `nest()`, `map()`, and `unnest()` into the preceding Baltimore-specific code, we fit a model for each of the cities, and extract the adjusted odds ratio (and CI) for solving homicides comparing non-white victims to white victims. We show the first 5 rows of the resulting dataframe of model results.

```{r q1_glm_all_cities}
model_results = 
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race, 
                             family = binomial(), data = df)),
    tidy_models = map(models, broom::tidy)) |> 
  select(-models, -data) |> 
  unnest(cols = tidy_models) |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(city_state, OR, OR_CI_lower, OR_CI_upper)

model_results |>
  slice(1:5) |> 
  knitr::kable(digits = 3)
```

Below we generate a plot of the estimated ORs and CIs for each city, ordered by magnitude of the OR from smallest to largest. From this plot we see that most cities have odds ratios that are smaller than 1, suggesting that crimes with male victims have smaller odds of resolution compared to crimes with female victims after adjusting for victim age and race. This disparity is strongest in New yrok. In roughly half of these cities, confidence intervals are narrow and do not contain 1, suggesting a significant difference in resolution rates by sex after adjustment for victim age and race. 

```{r q1_plot}
model_results |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```



### Problem 2


```{r}
## Download the Data:

weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())

```


```{r}
lm(tmax ~ tmin + prcp, data = weather_df)

bootstrap_sample = function(df) {
  sample_frac(df, replace=TRUE)
}

bootstraps = 
  tibble(strap_number = 1:5000) %>% 
  mutate(
    sample_strap = map(strap_number, ~bootstrap_sample(df = weather_df))
  )

bootstraps %>% 
  slice(1:3) %>% 
  mutate(sample_strap = map(sample_strap, arrange, tmax)) %>%  
  pull(sample_strap)

```


```{r}
# Fit our model on each sample 

results_bootstrap = 
  bootstraps %>%  
  mutate(
    models = map(sample_strap, ~lm(tmax ~ tmin + prcp, data = .)
  ), 
    results = map(models, broom::tidy),
    r_squared = map_dbl(models, ~broom::glance(.x)$r.squared),
    log_beta = map_dbl(models, ~ {
      coef <- coefficients(.x)
      if (coef[2] * coef[3] >= 0) log(coef[2] * coef[3]) else NA_real_
    })
  ) %>% 
  select(-sample_strap, -models) %>% 
  unnest(results)

```


Distribution Plots for r squared and log beta:

```{r}
# Distribution plot for R-squared
rs_estimate_plot = 
  results_bootstrap %>% 
ggplot(aes(x = r_squared)) +
  geom_density(color = "blue") +
  labs(x = "R-squared", y = "Density", title = "Distribution of R-squared")

rs_estimate_plot

# Distribution plot for log(beta)
logbeta_estimate_plot = 
  results_bootstrap %>% 
ggplot(aes(x = log_beta)) +
  geom_density(color = "red") +
  labs(x = "log(beta1 * beta2)", y = "Density", title = "Distribution of log(beta)")

logbeta_estimate_plot
```

The R squared plot appears to follow a normal distribution with slight leftward skew that is likely a product of random sampling variability. However, the log beta plot is highly skewed to the left which may be the result of there being negative beta estimates that were not able to be used for calculating the log estimates. This strong leftward skew suggests that these log beta estimates may not be representative of the underlying data set. 


95% Confidence Intervals: 

```{r}
# Calculate 95% confidence intervals for R squared and log beta
ci_rsquared = quantile(results_bootstrap$r_squared, c(0.025, 0.975))
ci_log_beta = quantile(results_bootstrap$log_beta, c(0.025, 0.975), na.rm = TRUE)

# Display confidence interval results
print(paste("95% CI for R-squared: [", round(ci_rsquared[1], 4), ",", round(ci_rsquared[2], 4), "]"))

print(paste("95% CI for log(beta1 * beta2): [", round(ci_log_beta[1], 4), ",", round(ci_log_beta[2], 4), "]"))

```



### Problem 3

Tidying the Data 
```{r}
# Load and clean the birthweight dataset
birthweight_df = 
read_csv("data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate(across(c(babysex, frace, malform, mrace), as_factor))  

# Check for missing data
missing_data <- birthweight_df %>%
  summarize_all(~ sum(is.na(.)))

```


Fitting a regression model
```{r}
# Fit a regression model for birthweight
fit_birthweight <- lm(bwt ~ ., data = birthweight_df)

step(fit_birthweight, direction = "both")

final_fit = lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthweight_df)

# Tidy up the output
final_fit %>%
  broom::tidy() %>%
  mutate(term = str_replace_all(term, ":", " * ")) %>%
  knitr::kable(digits = 3)

```

The birthweight prediction model was developed through a stepwise variable selection process, aiming to identify a parsimonious set of predictors that significantly contribute to explaining the variance in baby birthweight. The final model fit equation is: lm(bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthweight_df). The stepwise procedure involves iteratively adding or removing variables based on statistical criteria, ultimately leading to a model with a balanced trade-off between explanatory power and simplicity. The selection of these variables is based on their statistical significance in predicting birthweight within the given dataset. It is important to note that while other variables were considered during the stepwise procedure, the final model includes only those deemed to have a substantial impact on birthweight and aims to strike a balance between model complexity and predictive accuracy.



Plot of Model Residuals
```{r}
# Add predictions and residuals to the dataset
birthweight_res <- birthweight_df %>%
  modelr::add_predictions(fit_birthweight, var = ".fitted") %>%
  modelr::add_residuals(fit_birthweight, var = ".resid")

# Plot residuals against fitted values
ggplot(birthweight_res, aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.25, color = "blue") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(x = "Fitted Values", y = "Residuals") +
  theme_minimal()

```


Cross Validation: 
```{r}
# Model 1: Using length at birth and gestational age as predictors
fit_model_1 <- lm(bwt ~ blength + gaweeks, data = birthweight_df)

# Model 2: Using head circumference, length, sex, and interactions
fit_model_2 <- lm(bwt ~ bhead * blength * babysex, data = birthweight_df)

```


```{r}
birthweight_cv <- birthweight_df %>%
  select(bwt, blength, gaweeks, bhead, blength, babysex)

# Cross-validation for all three models
cv_results <- 
  birthweight_cv %>% 
  crossv_mc(100) %>%
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>%
  mutate(
    model_1_fit = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    model_2_fit = map(train, \(df) lm(bwt ~ (bhead + blength + babysex)^3, data = df)),
    model_initial_fit = map(train, \(df) lm(bwt ~ ., data = df)),
    rmse_model_1 = map2_dbl(model_1_fit, test, \(mod, df) rmse(mod, df)),
    rmse_model_2 = map2_dbl(model_2_fit, test, \(mod, df) rmse(mod, df)),
    rmse_initial_model = map2_dbl(model_initial_fit, test, \(mod, df) rmse(mod, df))
  )


# Compare the models
comparison_results <- 
  bind_rows(
    select(cv_results, rmse_model_1, rmse_model_2, rmse_initial_model)
  ) %>%
  pivot_longer(
    everything(),
    names_to = "model_type", 
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>%
  group_by(model_type) %>%
  summarize(m_rmse = mean(rmse))

print(comparison_results)
```

In summary, the initial model, which uses the predictors selected through the stepwise model building process, appears to provide the best average cross-validated prediction performance among the three models. Model 2, with additional predictors and interactions, performs better than Model 1 but just barely less well than the initial model. This comparison underscores the importance of considering the trade-off between model complexity and predictive accuracy.


